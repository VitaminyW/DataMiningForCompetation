{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'D:\\学习\\微热点大赛\\DataMiningForCompetation')\n",
    "from db import MyDB\n",
    "#加载数据库\n",
    "db = MyDB(host='182.61.54.181', user='root', password='yW88224646!', db='WRD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#加载使用库\n",
    "import pandas\n",
    "import os\n",
    "import numpy\n",
    "import jieba.posseg as pseg\n",
    "from multiprocessing import Pool,Process\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import threading"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def saveTempFile(path,Object_):\n",
    "    with open(path,'w') as f:\n",
    "        json.dump(Object_,f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# [是否为默认文本,内容文本,内容表情,地域,城市,性别,粉丝数,微博数,转,评,赞,话题,微博情绪,精准地域,中图地址,\n",
    "# MD5-作者ID,MD5-mid,MD5-根微博mid,MD5-根微博用户UID,MD5-父微博ID,MD5-父微博用户ID]\n",
    "targetRoot = r'E:\\Download\\数据大赛\\选题1\\初步分词\\\\'\n",
    "sourceRoot = r'E:\\Download\\数据大赛\\选题1\\微博'\n",
    "needPartOfSpeech = ['a','ad','an']\n",
    "tl1 = []\n",
    "emotion = re.compile(r'(\\[.*?\\])')\n",
    "tag = re.compile(r'(#.*?#)')\n",
    "tags = []\n",
    "emotions = []\n",
    "files = None\n",
    "for root,dirs,fs in os.walk(sourceRoot):\n",
    "    files = fs\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    fileAbsolutePath = sourceRoot+'\\\\'+file\n",
    "    dataFile = pandas.read_csv(fileAbsolutePath,encoding='gb18030')\n",
    "    tempList = []\n",
    "    for item in tqdm(dataFile.values):\n",
    "        temp = [int(item[0] == '转发微博 '),json.dumps(emotion.findall(item[0])), json.dumps(tag.findall(item[0])), item[0]]\n",
    "        for e in temp[1]:\n",
    "            temp[3] = temp[3].replace(e,'')\n",
    "        for t in temp[2]:\n",
    "            temp[3] = temp[3].replace(t,'')\n",
    "        if temp[3] == '转发微博 ':\n",
    "            temp[3] = ''\n",
    "        tempWords = []\n",
    "        words = pseg.cut(temp[3],use_paddle=True)\n",
    "        for word, flag in words:\n",
    "            if flag in needPartOfSpeech:\n",
    "                tempWords.append(word)\n",
    "        temp[3] = json.dumps(tempWords)\n",
    "        temp.append(int(item[1] == '敏感'))\n",
    "        temp.append(int(item[2] == '原创'))\n",
    "        for i in range(3,25):\n",
    "            if i == 9 or i == 4:\n",
    "                continue\n",
    "            temp.append(item[i])\n",
    "        tempList.append(temp)\n",
    "    tempList = pandas.DataFrame(tempList)\n",
    "    tempList.columns = ['是否为默认文本','内容表情','内容标签','过滤后内容分词json串','是否敏感','是否原创','发布日期','认证类型','地域','城市','性别','粉丝数','微博数','转','评','赞','话题','微博情绪','精准地域','中图地址',\n",
    "      'MD5-作者ID','MD5-mid','MD5-根微博mid','MD5-根微博用户UID','MD5-父微博ID','MD5-父微博用户ID']\n",
    "    tempList.to_csv(targetRoot+file,encoding='gb18030')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#读取一个csv文件进行分词结果分析\n",
    "test = pandas.read_csv(r'E:\\Download\\数据大赛\\选题1\\初步分词\\《乘风破浪的姐姐》成团之夜-2.csv',encoding='gb18030')\n",
    "for i in tqdm(range(test.shape[0])):\n",
    "    test.loc[i:i,'过滤后内容分词json串'] = str(json.loads(test['过滤后内容分词json串'][i]))\n",
    "test.to_csv(r'E:\\Download\\数据大赛\\选题1\\初步分词\\test.csv',encoding='gb18030')\n",
    "#第一部分尝试感受：目前最需要的应该是构建词表，反而不需要太过在意数据的其他属性"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:38<00:00,  5.19s/it]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'saveTempFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-3-b8ca279fd74d>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m     \u001B[0mWBEmotionList\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdataFile\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'微博情绪'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[0mWBEmotionList\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mWBEmotionList\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m \u001B[0msaveTempFile\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34mr'E:\\Download\\数据大赛\\选题1\\temp\\WBEmotionList.json'\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mWBEmotionList\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'saveTempFile' is not defined"
     ]
    }
   ],
   "source": [
    "#第二部分尝试将使用进行二分类数据预处理\n",
    "#1. 遍历数据，得到微博情绪的种类\n",
    "sourceRoot = r'E:\\Download\\数据大赛\\选题1\\微博'\n",
    "files = None\n",
    "WBEmotionList = list()\n",
    "for root,dirs,fs in os.walk(sourceRoot):\n",
    "    files = fs\n",
    "    break\n",
    "for file in tqdm(files):\n",
    "    fileAbsolutePath = sourceRoot+'\\\\'+file\n",
    "    dataFile = pandas.read_csv(fileAbsolutePath,encoding='gb18030')\n",
    "    WBEmotionList.extend(list(set(dataFile['微博情绪'].values.tolist())))\n",
    "WBEmotionList = list(set(WBEmotionList))\n",
    "saveTempFile(r'E:\\Download\\数据大赛\\选题1\\temp\\WBEmotionList.json',WBEmotionList)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "emotionsToThereEmotions = {'愤怒':'negative','恐惧':'negative','悲伤':'negative','中性':'neutral','喜悦':'positive','惊奇':'positive'}\n",
    "position = 0 #临界资源\n",
    "#定义线程工作\n",
    "max_Threading = 9\n",
    "threadLock = threading.Lock()\n",
    "\n",
    "def getNewFileName():\n",
    "    global position\n",
    "    fileName = files[position]\n",
    "    if position < 19:\n",
    "        position += 1\n",
    "        return fileName\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def workFunction(id):\n",
    "    nowFileName = None\n",
    "    emotion = re.compile(r'(\\[.*?\\])')\n",
    "    tag = re.compile(r'(#.*?#)')\n",
    "    while True:\n",
    "        threadLock.acquire()\n",
    "        nowFileName = getNewFileName()\n",
    "        threadLock.release()\n",
    "        print(nowFileName)\n",
    "        if nowFileName is None :\n",
    "            break\n",
    "        words = {'negative':[],'neutral':[],'positive':[]}\n",
    "        tempFile = pandas.read_csv(sourceRoot+'\\\\'+nowFileName,encoding='gb18030')\n",
    "        total = tempFile.shape[0]\n",
    "        for index,item in enumerate(tempFile.values.tolist()):\n",
    "           # if (index%1000) == 0:\n",
    "               # print(index/total)\n",
    "            oneSentenceWords = []\n",
    "            if item[0] == '转发微博 ':\n",
    "                oneSentenceWords.append('转发微博 ')\n",
    "                item[0].replace('转发微博 ','')\n",
    "            for e in emotion.findall(item[0]):\n",
    "                oneSentenceWords.append(e)\n",
    "                item[0].replace(e,'')\n",
    "            for t in tag.findall(item[0]):\n",
    "                oneSentenceWords.append(t)\n",
    "                item[0].replace(t,'')\n",
    "            w = pseg.cut(item[0],use_paddle=True)\n",
    "            for wd, fg in w:\n",
    "                oneSentenceWords.append(wd)\n",
    "            for j in oneSentenceWords:#将所有词记录进结构中\n",
    "                words[emotionsToThereEmotions[item[16]]].append(j)\n",
    "            if index%1000 == 0:#记录当前状态\n",
    "                with open(r'E:\\Download\\数据大赛\\选题1\\temp'+'\\\\'+nowFileName+'.txt','a+') as f:\n",
    "                    f.write(str((index/total))+'\\n')\n",
    "        with open(r'E:\\Download\\数据大赛\\选题1\\第二部分分词\\%d.txt'%id,'a+',encoding='gb18030') as f:\n",
    "            for key in words:\n",
    "                for wd in words[key]:\n",
    "                    f.write(wd+'\\t'+key+'\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020天猫双11总成交额4982亿元.csv\n",
      "3人伪造老干妈印章与腾讯签合同被刑拘.csv\n",
      "《乘风破浪的姐姐》成团之夜-1.csv《乘风破浪的姐姐》成团之夜-2.csv\n",
      "\n",
      "《网络直播营销活动行为规范》7月1日实施.csv\n",
      "上海首个小区停用丰巢快递柜.csv\n",
      "冒名顶替上大学拟写入刑法.csv\n",
      "司机为救婴儿连闯红灯家属拒绝作证.csv\n",
      "周杰伦王俊凯成为英雄联盟代言人-1.csv\n",
      "周杰伦王俊凯成为英雄联盟代言人-2.csv\n",
      "周杰伦王俊凯成为英雄联盟代言人-3.csv\n",
      "央视曝光汉堡王用过期面包做汉堡.csv\n",
      "拼多多首度实现盈利.csv\n",
      "武汉蝉联全国文明城市.csv\n",
      "汪涵李雪琴李佳琦直播被中消协点名.csv\n",
      "深圳调整商品住房限购年限.csv\n",
      "湖人获NBA总决赛冠军-1.csv\n",
      "湖人获NBA总决赛冠军-2.csv\n",
      "王勉获得《脱口秀大会》冠军.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\17797\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.948 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\ANACONDA\\envs\\untitled\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"D:\\ANACONDA\\envs\\untitled\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-3-406dcc954a4c>\", line 22, in workFunction\n",
      "    nowFileName = getNewFileName()\n",
      "  File \"<ipython-input-3-406dcc954a4c>\", line 9, in getNewFileName\n",
      "    fileName = files[position]\n",
      "IndexError: list index out of range\n",
      "\n"
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "for i in range(max_Threading):\n",
    "    threads.append(threading.Thread(target=workFunction,args=[i]))\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "for thread in threads:\n",
    "    thread.join()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# 第三部分 统计词频\n",
    "with open(r'E:\\Download\\数据大赛\\选题1\\第二部分分词\\8.txt',encoding='gb18030') as f:\n",
    "    data = f.read()\n",
    "data = data.split('\\n')\n",
    "for i,item in tqdm(enumerate(data])):\n",
    "        data[i] = data[i].split('\\t')\n",
    "data.pop(-1)\n",
    "for i,item in tqdm(enumerate(data2)):\n",
    "        data2[i] = data2[i].split('\\t')\n",
    "data2.sort(key=lambda item:item[0])\n",
    "clazz = {'negative':[],'neutral':[],'positive':[]}\n",
    "for item in tqdm(data2):\n",
    "    try:\n",
    "       clazz[item[1]].append(item[0])\n",
    "    except:\n",
    "        continue\n",
    "freCount = {'negative':[],'neutral':[],'positive':[]}\n",
    "for key in clazz:\n",
    "    if len(clazz[key]) == 0:\n",
    "        continue\n",
    "    count = 1\n",
    "    nowWord = clazz[key][0]\n",
    "    for i in tqdm(range(1,len(clazz[key])),desc=key):\n",
    "        if clazz[key][i] == nowWord:\n",
    "            count+=1\n",
    "        else:\n",
    "            freCount[key].append([nowWord,count])\n",
    "            count = 1\n",
    "            nowWord = clazz[key][i]\n",
    "            if i == len(clazz[key])-1:\n",
    "                freCount[key].append([nowWord,count])\n",
    "        if clazz[key][i] == nowWord and i == len(clazz[key])-1:\n",
    "            freCount[key].append([nowWord,count])\n",
    "with open(r'E:\\Download\\数据大赛\\选题1\\第二部分分词\\9-fre.json', 'w', encoding='gb18030') as f:\n",
    "    json.dump(freCount, f)\n",
    "del freCount"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "freCounts = []\n",
    "for i in range(0,10):\n",
    "    with open(r'E:\\Download\\数据大赛\\选题1\\第二部分分词\\%d-fre.json'%i, encoding='gb18030') as f:\n",
    "        freCounts.append(json.load(f))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "totalFreCount = {'negative':{},'neutral':{},'positive':{}}\n",
    "for item in freCounts:\n",
    "    for key in item:\n",
    "        for j in item[key]:\n",
    "            if j[0] in totalFreCount[key]:\n",
    "                totalFreCount[key][j[0]] = totalFreCount[key][j[0]] + j[1]\n",
    "            else:\n",
    "                totalFreCount[key].update({j[0]:j[1]})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                         | 1938/189575 [01:19<2:04:06, 25.20it/s]"
     ]
    }
   ],
   "source": [
    "for key in totalFreCount:\n",
    "    for key2 in tqdm(totalFreCount[key]):\n",
    "        db.insert_data({'word':key2,'count_':totalFreCount[key][key2],'sentiment':key},'分词词频2')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}