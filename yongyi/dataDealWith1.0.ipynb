{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#导入包\n",
    "import pandas as pd\n",
    "\n",
    "import jieba\n",
    "import jieba.analyse as ana\n",
    "import jieba.posseg as psg\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pymysql\n",
    "import re\n",
    "from itertools import chain\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据个数： 221902 -> 218591\n",
      "数据个数： 392935 -> 390198\n",
      "数据个数： 689909 -> 639337\n",
      "数据个数： 360625 -> 339905\n",
      "数据个数： 1830 -> 1807\n",
      "数据个数： 39914 -> 39502\n",
      "数据个数： 17314 -> 14789\n",
      "数据个数： 87024 -> 85933\n",
      "数据个数： 749899 -> 393161\n",
      "数据个数： 749917 -> 368273\n",
      "数据个数： 519264 -> 230253\n",
      "数据个数： 76049 -> 75320\n",
      "数据个数： 3654 -> 3584\n",
      "数据个数： 2426 -> 2284\n",
      "数据个数： 2226 -> 2219\n",
      "数据个数： 16994 -> 16803\n",
      "数据个数： 698270 -> 673898\n",
      "数据个数： 491431 -> 473720\n",
      "数据个数： 136483 -> 128039\n"
     ]
    }
   ],
   "source": [
    "#根据 用户id，微博内容，情绪 对数据进行去重，\n",
    "targetRoot = r'D:\\数据挖掘比赛\\选题1\\去重数据\\\\'\n",
    "sourceRoot = r'D:\\数据挖掘比赛\\选题1\\微博'\n",
    "files = None\n",
    "for root,dirs,fs in os.walk(sourceRoot):\n",
    "    files = fs\n",
    "    break\n",
    "for file in files:\n",
    "    fileAbsolutePath = sourceRoot+'\\\\'+file\n",
    "    data = pd.read_csv(fileAbsolutePath,encoding='gb18030')\n",
    "    NoDuplicateData = data.drop_duplicates([\"全文内容\",\"微博情绪\",\"MD5-作者ID\"],keep=\"first\")\n",
    "    NoDuplicateData.to_csv(targetRoot+file,encoding=\"gb18030\")\n",
    "    # print(\"数据个数：\",len(data),\"->\",len(NoDuplicateData))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据长度： 218591 -> 57353\n",
      "分类长度索引 [21040, 6994, 6825, 17699, 3613, 1188]\n",
      "数据长度： 390198 -> 258171\n",
      "分类长度索引 [95725, 43556, 54814, 51460, 10467, 2155]\n",
      "数据长度： 639337 -> 620070\n",
      "分类长度索引 [314250, 121722, 72920, 90716, 14195, 6273]\n",
      "数据长度： 339905 -> 827273\n",
      "分类长度索引 [440023, 166606, 82783, 112931, 16238, 8698]\n",
      "数据长度： 1807 -> 828320\n",
      "分类长度索引 [440258, 166634, 83061, 113412, 16239, 8722]\n",
      "数据长度： 39502 -> 850600\n",
      "分类长度索引 [446893, 169877, 89222, 118681, 16656, 9277]\n",
      "数据长度： 14789 -> 858058\n",
      "分类长度索引 [447967, 170316, 90600, 123065, 16767, 9349]\n",
      "数据长度： 85933 -> 904044\n",
      "分类长度索引 [455502, 172553, 106129, 139377, 20386, 10103]\n",
      "数据长度： 393161 -> 1027372\n",
      "分类长度索引 [546102, 199037, 107813, 143448, 20541, 10437]\n",
      "数据长度： 368273 -> 1136122\n",
      "分类长度索引 [624014, 224431, 109233, 147148, 20660, 10642]\n",
      "数据长度： 230253 -> 1199915\n",
      "分类长度索引 [666518, 242848, 109885, 149239, 20710, 10721]\n",
      "数据长度： 75320 -> 1238742\n",
      "分类长度索引 [686352, 251257, 114463, 153754, 20990, 11932]\n",
      "数据长度： 3584 -> 1240652\n",
      "分类长度索引 [686621, 251300, 114575, 155102, 21060, 12000]\n",
      "数据长度： 2284 -> 1241794\n",
      "分类长度索引 [686857, 251315, 114757, 155793, 21076, 12002]\n",
      "数据长度： 2219 -> 1243451\n",
      "分类长度索引 [687308, 251513, 114916, 156253, 21348, 12119]\n",
      "数据长度： 16803 -> 1254076\n",
      "分类长度索引 [689623, 252639, 117139, 161022, 21403, 12256]\n",
      "数据长度： 673898 -> 1566625\n",
      "分类长度索引 [827108, 296413, 146628, 252438, 29065, 14979]\n",
      "数据长度： 473720 -> 1805554\n",
      "分类长度索引 [958162, 337883, 163125, 296267, 33860, 16263]\n",
      "数据长度： 128039 -> 1876056\n",
      "分类长度索引 [993800, 350080, 171046, 307858, 36536, 16742]\n"
     ]
    }
   ],
   "source": [
    "#把所有数据按照情绪进行分类,并进行分词和词性标注，存入csv文件中\n",
    "#【全文内容，情绪，jieba分词，jieba词性标注】\n",
    "emotionRoot = r'D:\\数据挖掘比赛\\选题1\\情绪分类数据'\n",
    "NoDuplicateRoot = r'D:\\数据挖掘比赛\\选题1\\去重数据'\n",
    "files = None\n",
    "indexlist = [1,1,1,1,1,1]\n",
    "leng = 0    #记录去重后数据长度\n",
    "for root,dirs,fs in os.walk(NoDuplicateRoot):\n",
    "    files = fs\n",
    "    break\n",
    "for id,file in enumerate(files):\n",
    "    fileAbsolutePath = NoDuplicateRoot+'\\\\'+file\n",
    "    data = pd.read_csv(fileAbsolutePath,encoding='gb18030')\n",
    "    Gdata = data.groupby(by='微博情绪')  # 根据微博情绪进行分组\n",
    "    PosData = Gdata.get_group('喜悦').loc[:, '全文内容']   #提取出对应内容\n",
    "    NegData = Gdata.get_group('悲伤').loc[:, '全文内容']\n",
    "    angerData = Gdata.get_group('愤怒').loc[:, '全文内容']\n",
    "    middleData = Gdata.get_group('中性').loc[:, '全文内容']\n",
    "    amazeData = Gdata.get_group('惊奇').loc[:, '全文内容']\n",
    "    fearData = Gdata.get_group('恐惧').loc[:, '全文内容']\n",
    "    emotionData = [PosData,NegData,angerData,middleData,amazeData,fearData]\n",
    "    emotion = [\"喜悦\",\"悲伤\",\"愤怒\",\"中性\",\"惊奇\",\"恐惧\"]\n",
    "\n",
    "    #emotionData.append()\n",
    "    for idx,emotionList in enumerate(emotionData):\n",
    "        emotionList = emotionList.to_frame()    ##将series变成dataframe类型\n",
    "        emotionList = emotionList.drop_duplicates()     ##去掉完全重复的数据\n",
    "        leng += len(emotionList)\n",
    "        emotionList[\"分词\"] = emotionList[\"全文内容\"].apply(jieba.lcut)\n",
    "        emotionList[\"词性标注\"] = emotionList[\"全文内容\"].apply(psg.cut)\n",
    "        a = indexlist[idx] + len(emotionList)       #让存入csv文件的序号有序，从1开始\n",
    "        emotionList.index = np.arange(indexlist[idx],a)\n",
    "        indexlist[idx] = a\n",
    "        if(id == 0):\n",
    "            emotionList.to_csv(emotionRoot+'\\\\'+emotion[idx]+'.csv',encoding=\"gb18030\",mode=\"a\")\n",
    "        else:\n",
    "            emotionList.to_csv(emotionRoot+'\\\\'+emotion[idx]+'.csv',encoding=\"gb18030\",mode=\"a\",header=False)\n",
    "    print(\"数据长度：\",len(data),\"->\",leng)\n",
    "    print(\"分类长度索引\",indexlist)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264\n",
      "[ ' 明 年 ' ,   ' 怕 ' ,   ' 不 是 ' ,   ' 要 ' ,   ' 破 ' ,   ' 五 千 ' ,   ' 亿 ' ,   '   ' ,   ' 【 ' ,   ' 原 微 博 ' ,   ' 】 ' ,   '   ' ,   ' @ ' ,   ' 天 下 网 ' ,   ' 商 ' ,   '   ' ,   ' 4 9 8 2 ' ,   ' 亿 ' ,   ' ！ ' ,   ' 截 至 ' ,   ' 1 1 ' ,   ' 月 ' ,   ' 1 1 ' ,   ' 日 ' ,   ' 2 4 ' ,   ' 时 ' ,   ' ， ' ,   ' 2 0 2 0 ' ,   ' 年 天 ' ,   ' 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' 全 球 ' ,   ' 狂 欢 ' ,   ' 季 ' ,   ' 成 交 额 ' ,   ' 4 9 8 2 ' ,   ' 亿 元 ' ,   ' 。 ' ,   ' # ' ,   ' 天 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' # ' ,   '   ' ,   ' ? ' ,   ' ? ' ]\n",
      "[\"\\u660e\", \"\\u5e74\", \"\\u6015\", \"\\u4e0d\", \"\\u662f\", \"\\u8981\", \"\\u7834\", \"\\u4e94\", \"\\u5343\", \"\\u4ebf\", \"\\u539f\", \"\\u5fae\", \"\\u535a\", \"\\u5929\", \"\\u4e0b\", \"\\u7f51\", \"\\u5546\", \"\\u4ebf\", \"\\u622a\", \"\\u81f3\", \"\\u6708\", \"\\u65e5\", \"\\u65f6\", \"\\u5e74\", \"\\u5929\", \"\\u732b\", \"\\u53cc\", \"\\u5168\", \"\\u7403\", \"\\u72c2\", \"\\u6b22\", \"\\u5b63\", \"\\u6210\", \"\\u4ea4\", \"\\u989d\", \"\\u4ebf\", \"\\u5143\", \"\\u5929\", \"\\u732b\", \"\\u53cc\"]\n",
      "264\n",
      "[ ' 明 年 ' ,   ' 怕 ' ,   ' 不 是 ' ,   ' 要 ' ,   ' 破 ' ,   ' 五 千 ' ,   ' 亿 ' ,   '   ' ,   ' 【 ' ,   ' 原 微 博 ' ,   ' 】 ' ,   '   ' ,   ' @ ' ,   ' 天 下 网 ' ,   ' 商 ' ,   '   ' ,   ' 4 9 8 2 ' ,   ' 亿 ' ,   ' ！ ' ,   ' 截 至 ' ,   ' 1 1 ' ,   ' 月 ' ,   ' 1 1 ' ,   ' 日 ' ,   ' 2 4 ' ,   ' 时 ' ,   ' ， ' ,   ' 2 0 2 0 ' ,   ' 年 天 ' ,   ' 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' 全 球 ' ,   ' 狂 欢 ' ,   ' 季 ' ,   ' 成 交 额 ' ,   ' 4 9 8 2 ' ,   ' 亿 元 ' ,   ' 。 ' ,   ' # ' ,   ' 天 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' # ' ,   '   ' ,   ' ? ' ,   ' ? ' ]\n",
      "[\"\\u660e\", \"\\u5e74\", \"\\u6015\", \"\\u4e0d\", \"\\u662f\", \"\\u8981\", \"\\u7834\", \"\\u4e94\", \"\\u5343\", \"\\u4ebf\", \"\\u539f\", \"\\u5fae\", \"\\u535a\", \"\\u5929\", \"\\u4e0b\", \"\\u7f51\", \"\\u5546\", \"\\u4ebf\", \"\\u622a\", \"\\u81f3\", \"\\u6708\", \"\\u65e5\", \"\\u65f6\", \"\\u5e74\", \"\\u5929\", \"\\u732b\", \"\\u53cc\", \"\\u5168\", \"\\u7403\", \"\\u72c2\", \"\\u6b22\", \"\\u5b63\", \"\\u6210\", \"\\u4ea4\", \"\\u989d\", \"\\u4ebf\", \"\\u5143\", \"\\u5929\", \"\\u732b\", \"\\u53cc\"]\n",
      "264\n",
      "[ ' 明 年 ' ,   ' 怕 ' ,   ' 不 是 ' ,   ' 要 ' ,   ' 破 ' ,   ' 五 千 ' ,   ' 亿 ' ,   '   ' ,   ' 【 ' ,   ' 原 微 博 ' ,   ' 】 ' ,   '   ' ,   ' @ ' ,   ' 天 下 网 ' ,   ' 商 ' ,   '   ' ,   ' 4 9 8 2 ' ,   ' 亿 ' ,   ' ！ ' ,   ' 截 至 ' ,   ' 1 1 ' ,   ' 月 ' ,   ' 1 1 ' ,   ' 日 ' ,   ' 2 4 ' ,   ' 时 ' ,   ' ， ' ,   ' 2 0 2 0 ' ,   ' 年 天 ' ,   ' 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' 全 球 ' ,   ' 狂 欢 ' ,   ' 季 ' ,   ' 成 交 额 ' ,   ' 4 9 8 2 ' ,   ' 亿 元 ' ,   ' 。 ' ,   ' # ' ,   ' 天 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' # ' ,   '   ' ,   ' ? ' ,   ' ? ' ]\n",
      "[\"\\u660e\", \"\\u5e74\", \"\\u6015\", \"\\u4e0d\", \"\\u662f\", \"\\u8981\", \"\\u7834\", \"\\u4e94\", \"\\u5343\", \"\\u4ebf\", \"\\u539f\", \"\\u5fae\", \"\\u535a\", \"\\u5929\", \"\\u4e0b\", \"\\u7f51\", \"\\u5546\", \"\\u4ebf\", \"\\u622a\", \"\\u81f3\", \"\\u6708\", \"\\u65e5\", \"\\u65f6\", \"\\u5e74\", \"\\u5929\", \"\\u732b\", \"\\u53cc\", \"\\u5168\", \"\\u7403\", \"\\u72c2\", \"\\u6b22\", \"\\u5b63\", \"\\u6210\", \"\\u4ea4\", \"\\u989d\", \"\\u4ebf\", \"\\u5143\", \"\\u5929\", \"\\u732b\", \"\\u53cc\"]\n",
      "264\n",
      "[ ' 明 年 ' ,   ' 怕 ' ,   ' 不 是 ' ,   ' 要 ' ,   ' 破 ' ,   ' 五 千 ' ,   ' 亿 ' ,   '   ' ,   ' 【 ' ,   ' 原 微 博 ' ,   ' 】 ' ,   '   ' ,   ' @ ' ,   ' 天 下 网 ' ,   ' 商 ' ,   '   ' ,   ' 4 9 8 2 ' ,   ' 亿 ' ,   ' ！ ' ,   ' 截 至 ' ,   ' 1 1 ' ,   ' 月 ' ,   ' 1 1 ' ,   ' 日 ' ,   ' 2 4 ' ,   ' 时 ' ,   ' ， ' ,   ' 2 0 2 0 ' ,   ' 年 天 ' ,   ' 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' 全 球 ' ,   ' 狂 欢 ' ,   ' 季 ' ,   ' 成 交 额 ' ,   ' 4 9 8 2 ' ,   ' 亿 元 ' ,   ' 。 ' ,   ' # ' ,   ' 天 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' # ' ,   '   ' ,   ' ? ' ,   ' ? ' ]\n",
      "[\"\\u660e\", \"\\u5e74\", \"\\u6015\", \"\\u4e0d\", \"\\u662f\", \"\\u8981\", \"\\u7834\", \"\\u4e94\", \"\\u5343\", \"\\u4ebf\", \"\\u539f\", \"\\u5fae\", \"\\u535a\", \"\\u5929\", \"\\u4e0b\", \"\\u7f51\", \"\\u5546\", \"\\u4ebf\", \"\\u622a\", \"\\u81f3\", \"\\u6708\", \"\\u65e5\", \"\\u65f6\", \"\\u5e74\", \"\\u5929\", \"\\u732b\", \"\\u53cc\", \"\\u5168\", \"\\u7403\", \"\\u72c2\", \"\\u6b22\", \"\\u5b63\", \"\\u6210\", \"\\u4ea4\", \"\\u989d\", \"\\u4ebf\", \"\\u5143\", \"\\u5929\", \"\\u732b\", \"\\u53cc\"]\n",
      "264\n",
      "[ ' 明 年 ' ,   ' 怕 ' ,   ' 不 是 ' ,   ' 要 ' ,   ' 破 ' ,   ' 五 千 ' ,   ' 亿 ' ,   '   ' ,   ' 【 ' ,   ' 原 微 博 ' ,   ' 】 ' ,   '   ' ,   ' @ ' ,   ' 天 下 网 ' ,   ' 商 ' ,   '   ' ,   ' 4 9 8 2 ' ,   ' 亿 ' ,   ' ！ ' ,   ' 截 至 ' ,   ' 1 1 ' ,   ' 月 ' ,   ' 1 1 ' ,   ' 日 ' ,   ' 2 4 ' ,   ' 时 ' ,   ' ， ' ,   ' 2 0 2 0 ' ,   ' 年 天 ' ,   ' 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' 全 球 ' ,   ' 狂 欢 ' ,   ' 季 ' ,   ' 成 交 额 ' ,   ' 4 9 8 2 ' ,   ' 亿 元 ' ,   ' 。 ' ,   ' # ' ,   ' 天 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' # ' ,   '   ' ,   ' ? ' ,   ' ? ' ]\n",
      "[\"\\u660e\", \"\\u5e74\", \"\\u6015\", \"\\u4e0d\", \"\\u662f\", \"\\u8981\", \"\\u7834\", \"\\u4e94\", \"\\u5343\", \"\\u4ebf\", \"\\u539f\", \"\\u5fae\", \"\\u535a\", \"\\u5929\", \"\\u4e0b\", \"\\u7f51\", \"\\u5546\", \"\\u4ebf\", \"\\u622a\", \"\\u81f3\", \"\\u6708\", \"\\u65e5\", \"\\u65f6\", \"\\u5e74\", \"\\u5929\", \"\\u732b\", \"\\u53cc\", \"\\u5168\", \"\\u7403\", \"\\u72c2\", \"\\u6b22\", \"\\u5b63\", \"\\u6210\", \"\\u4ea4\", \"\\u989d\", \"\\u4ebf\", \"\\u5143\", \"\\u5929\", \"\\u732b\", \"\\u53cc\"]\n",
      "264\n",
      "[ ' 明 年 ' ,   ' 怕 ' ,   ' 不 是 ' ,   ' 要 ' ,   ' 破 ' ,   ' 五 千 ' ,   ' 亿 ' ,   '   ' ,   ' 【 ' ,   ' 原 微 博 ' ,   ' 】 ' ,   '   ' ,   ' @ ' ,   ' 天 下 网 ' ,   ' 商 ' ,   '   ' ,   ' 4 9 8 2 ' ,   ' 亿 ' ,   ' ！ ' ,   ' 截 至 ' ,   ' 1 1 ' ,   ' 月 ' ,   ' 1 1 ' ,   ' 日 ' ,   ' 2 4 ' ,   ' 时 ' ,   ' ， ' ,   ' 2 0 2 0 ' ,   ' 年 天 ' ,   ' 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' 全 球 ' ,   ' 狂 欢 ' ,   ' 季 ' ,   ' 成 交 额 ' ,   ' 4 9 8 2 ' ,   ' 亿 元 ' ,   ' 。 ' ,   ' # ' ,   ' 天 猫 ' ,   ' 双 ' ,   ' 1 1 ' ,   ' # ' ,   '   ' ,   ' ? ' ,   ' ? ' ]\n",
      "[\"\\u660e\", \"\\u5e74\", \"\\u6015\", \"\\u4e0d\", \"\\u662f\", \"\\u8981\", \"\\u7834\", \"\\u4e94\", \"\\u5343\", \"\\u4ebf\", \"\\u539f\", \"\\u5fae\", \"\\u535a\", \"\\u5929\", \"\\u4e0b\", \"\\u7f51\", \"\\u5546\", \"\\u4ebf\", \"\\u622a\", \"\\u81f3\", \"\\u6708\", \"\\u65e5\", \"\\u65f6\", \"\\u5e74\", \"\\u5929\", \"\\u732b\", \"\\u53cc\", \"\\u5168\", \"\\u7403\", \"\\u72c2\", \"\\u6b22\", \"\\u5b63\", \"\\u6210\", \"\\u4ea4\", \"\\u989d\", \"\\u4ebf\", \"\\u5143\", \"\\u5929\", \"\\u732b\", \"\\u53cc\"]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding = utf-8 -*-\n",
    "#对词性标注数据进行词频统计（利用正则表达式去除非中文符号），放进数据库表格中，【词，词性，词频】\n",
    "emotionRoot = r'D:\\数据挖掘比赛\\选题1\\情绪分类数据'\n",
    "for root,dirs,fs in os.walk(emotionRoot):\n",
    "    files = fs\n",
    "    break\n",
    "for id,file in enumerate(files):\n",
    "    fileAbsolutePath = emotionRoot+'\\\\'+file\n",
    "    data = pd.read_csv('D:\\数据挖掘比赛\\选题1\\情绪分类数据\\恐惧.csv',encoding='gb18030')\n",
    "    # print(type(data))\n",
    "    # print(data[\"分词\"][0])\n",
    "    # for i in data[\"分词\"][0]:\n",
    "    #     print(i)\n",
    "    # print(type(data))\n",
    "    # data = ['我们']\n",
    "    # print()\n",
    "    print(len(data[\"分词\"][0]))\n",
    "    pattern = \"[\\u4e00-\\u9fa5]+\"    #中文正则表达式\n",
    "    regex = re.compile(pattern)     #正则对象\n",
    "    print(' '.join(data[\"分词\"][0]))\n",
    "    print(json.dumps(regex.findall(' '.join(data[\"分词\"][0]))))\n",
    "    # data[\"分词\"] = data[\"分词\"].apply(lambda x: regex.findall(' '.join(x)))\n",
    "\n",
    "    # print(data[\"分词\"])#\n",
    "    # wordNum = []\n",
    "    # # for i in data[\"分词\"]:\n",
    "    # #     wordNum = wordNum+i\n",
    "    # datalist = data[\"分词\"].values\n",
    "    # datalist = list(chain.from_iterable(datalist))\n",
    "    # datalist = list(chain.from_iterable(datalist))\n",
    "    # fdist = FreqDist(datalist)\n",
    "    # dbConnect = pymysql.connect(host='182.61.54.181',port=3306,db='WRD',user='yonYi',password='YonYi12!')\n",
    "    # dbCursor = dbConnect.cursor()   #游标对象\n",
    "    # table_name = file.strip(\".csv\") + \"jieba分词词频\"\n",
    "    # dbCursor.execute(\"DROP TABLE IF EXISTS %s\"%(table_name))\n",
    "    # # 使用预处理语句创建表\n",
    "    # sql = \"\"\"CREATE TABLE  %s(\n",
    "    #         word  CHAR(100),\n",
    "    #         num INT)\n",
    "    #         \"\"\"%(table_name)#表名为变量\n",
    "    # dbCursor.execute(sql)\n",
    "    # for word,num in fdist.most_common():\n",
    "    #     sql1 = '''\n",
    "    #         insert into  %s(\n",
    "    #             word,num\n",
    "    #         )\n",
    "    #         values(\"%s\",\"%d\")\n",
    "    #     '''%(table_name,word,num)    #values里面的%s尽量用双引号\n",
    "    #     dbCursor.execute(sql1)\n",
    "    #     # # sql = \"SELECT * FROM table_name ORDER BY name DESC\"\n",
    "    #     # sql2 = '''\n",
    "    #     #     SELECT * FROM %s ORDER BY num DESC\n",
    "    #     # '''%(table_name)    #values里面的%s尽量用双引号\n",
    "    #     # dbCursor.execute(sql2)\n",
    "    #     dbConnect.commit()      #提交\n",
    "    # dbCursor.close()\n",
    "    # dbConnect.close()\n",
    "    # print(file.strip(\".csv\") + \"表已创建！\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#jieba分词数据进行词频统计，放进数据库表格中。【词，词频】\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#取出词性标注数据，去除名词，标点符号等无用词，进行词的向量化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#模型训练"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}